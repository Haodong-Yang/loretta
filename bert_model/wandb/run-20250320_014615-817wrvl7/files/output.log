Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "/scratch/pawsey1001/haodongyang/loretta/bert_model/run_glue_v5.py", line 415, in <module>
    main()
  File "/scratch/pawsey1001/haodongyang/loretta/bert_model/run_glue_v5.py", line 187, in main
    model = get_peft_model(model, peft_config)
  File "/scratch/pawsey1001/haodongyang/ViT-FT/peft/src/peft/mapping_func.py", line 123, in get_peft_model
    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](
  File "/scratch/pawsey1001/haodongyang/ViT-FT/peft/src/peft/peft_model.py", line 1486, in __init__
    super().__init__(model, peft_config, adapter_name, **kwargs)
  File "/scratch/pawsey1001/haodongyang/ViT-FT/peft/src/peft/peft_model.py", line 132, in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
  File "/scratch/pawsey1001/haodongyang/ViT-FT/peft/src/peft/tuners/lora/model.py", line 142, in __init__
    super().__init__(model, config, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
  File "/scratch/pawsey1001/haodongyang/ViT-FT/peft/src/peft/tuners/tuners_utils.py", line 180, in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
  File "/scratch/pawsey1001/haodongyang/ViT-FT/peft/src/peft/tuners/tuners_utils.py", line 516, in inject_adapter
    raise ValueError(error_msg)
ValueError: Target modules {'value', 'query'} not found in the base model. Please check the target modules and try again.
Traceback (most recent call last):
  File "/scratch/pawsey1001/haodongyang/loretta/bert_model/run_glue_v5.py", line 415, in <module>
    main()
  File "/scratch/pawsey1001/haodongyang/loretta/bert_model/run_glue_v5.py", line 187, in main
    model = get_peft_model(model, peft_config)
  File "/scratch/pawsey1001/haodongyang/ViT-FT/peft/src/peft/mapping_func.py", line 123, in get_peft_model
    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](
  File "/scratch/pawsey1001/haodongyang/ViT-FT/peft/src/peft/peft_model.py", line 1486, in __init__
    super().__init__(model, peft_config, adapter_name, **kwargs)
  File "/scratch/pawsey1001/haodongyang/ViT-FT/peft/src/peft/peft_model.py", line 132, in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
  File "/scratch/pawsey1001/haodongyang/ViT-FT/peft/src/peft/tuners/lora/model.py", line 142, in __init__
    super().__init__(model, config, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
  File "/scratch/pawsey1001/haodongyang/ViT-FT/peft/src/peft/tuners/tuners_utils.py", line 180, in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
  File "/scratch/pawsey1001/haodongyang/ViT-FT/peft/src/peft/tuners/tuners_utils.py", line 516, in inject_adapter
    raise ValueError(error_msg)
ValueError: Target modules {'value', 'query'} not found in the base model. Please check the target modules and try again.
DebertaV2ForSequenceClassification(
  (deberta): DebertaV2Model(
    (embeddings): DebertaV2Embeddings(
      (word_embeddings): Embedding(128100, 768, padding_idx=0)
      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): DebertaV2Encoder(
      (layer): ModuleList(
        (0-5): 6 x DebertaV2Layer(
          (attention): DebertaV2Attention(
            (self): DisentangledSelfAttention(
              (query_proj): Linear(in_features=768, out_features=768, bias=True)
              (key_proj): Linear(in_features=768, out_features=768, bias=True)
              (value_proj): Linear(in_features=768, out_features=768, bias=True)
              (pos_dropout): Dropout(p=0.1, inplace=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): DebertaV2SelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): DebertaV2Intermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): DebertaV2Output(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (rel_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
    )
  )
  (pooler): ContextPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0, inplace=False)
  )
  (classifier): Linear(in_features=768, out_features=2, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
)