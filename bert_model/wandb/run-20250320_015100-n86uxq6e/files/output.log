Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
DebertaV2ForSequenceClassification(
  (deberta): DebertaV2Model(
    (embeddings): DebertaV2Embeddings(
      (word_embeddings): Embedding(128100, 768, padding_idx=0)
      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): DebertaV2Encoder(
      (layer): ModuleList(
        (0-5): 6 x DebertaV2Layer(
          (attention): DebertaV2Attention(
            (self): DisentangledSelfAttention(
              (query_proj): Linear(in_features=768, out_features=768, bias=True)
              (key_proj): Linear(in_features=768, out_features=768, bias=True)
              (value_proj): Linear(in_features=768, out_features=768, bias=True)
              (pos_dropout): Dropout(p=0.1, inplace=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): DebertaV2SelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): DebertaV2Intermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): DebertaV2Output(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (rel_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
    )
  )
  (pooler): ContextPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0, inplace=False)
  )
  (classifier): Linear(in_features=768, out_features=2, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
PeftModelForSequenceClassification(
  (base_model): LoraModel(
    (model): DebertaV2ForSequenceClassification(
      (deberta): DebertaV2Model(
        (embeddings): DebertaV2Embeddings(
          (word_embeddings): Embedding(128100, 768, padding_idx=0)
          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): DebertaV2Encoder(
          (layer): ModuleList(
            (0-5): 6 x DebertaV2Layer(
              (attention): DebertaV2Attention(
                (self): DisentangledSelfAttention(
                  (query_proj): lora.Linear(
                    (base_layer): Linear(in_features=768, out_features=768, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Identity()
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=768, out_features=8, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=8, out_features=768, bias=False)
                    )
                    (lora_E): ModuleDict(
                      (default): Linear(in_features=8, out_features=1, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                    (lora_magnitude_vector): ModuleDict()
                  )
                  (key_proj): Linear(in_features=768, out_features=768, bias=True)
                  (value_proj): lora.Linear(
                    (base_layer): Linear(in_features=768, out_features=768, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Identity()
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=768, out_features=8, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=8, out_features=768, bias=False)
                    )
                    (lora_E): ModuleDict(
                      (default): Linear(in_features=8, out_features=1, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                    (lora_magnitude_vector): ModuleDict()
                  )
                  (pos_dropout): Dropout(p=0.1, inplace=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): DebertaV2SelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): DebertaV2Intermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): DebertaV2Output(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (rel_embeddings): Embedding(512, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
        )
      )
      (pooler): ContextPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (dropout): Dropout(p=0, inplace=False)
      )
      (classifier): ModulesToSaveWrapper(
        (original_module): Linear(in_features=768, out_features=2, bias=True)
        (modules_to_save): ModuleDict(
          (default): Linear(in_features=768, out_features=2, bias=True)
        )
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)
name base_model.model.deberta.encoder.layer.0.attention.self.query_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.0.attention.self.query_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.0.attention.self.query_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.0.attention.self.value_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.0.attention.self.value_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.0.attention.self.value_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.1.attention.self.query_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.1.attention.self.query_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.1.attention.self.query_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.1.attention.self.value_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.1.attention.self.value_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.1.attention.self.value_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.2.attention.self.query_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.2.attention.self.query_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.2.attention.self.query_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.2.attention.self.value_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.2.attention.self.value_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.2.attention.self.value_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.3.attention.self.query_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.3.attention.self.query_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.3.attention.self.query_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.3.attention.self.value_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.3.attention.self.value_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.3.attention.self.value_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.4.attention.self.query_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.4.attention.self.query_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.4.attention.self.query_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.4.attention.self.value_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.4.attention.self.value_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.4.attention.self.value_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.5.attention.self.query_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.5.attention.self.query_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.5.attention.self.query_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.5.attention.self.value_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.5.attention.self.value_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.5.attention.self.value_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.classifier.modules_to_save.default.weight shape torch.Size([2, 768]) dtype torch.float32
name base_model.model.classifier.modules_to_save.default.bias shape torch.Size([2]) dtype torch.float32
/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 67349/67349 [00:03<00:00, 18094.35 examples/s]
Map:   0%|                                                                                                                                                                                                          | 0/872 [00:00<?, ? examples/s]
Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 872/872 [00:00<00:00, 16343.81 examples/s]
Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1821/1821 [00:00<00:00, 17058.24 examples/s]
No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
[34m[1mwandb[39m[22m: [33mWARNING[39m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
  0%|                                                                                                                                                                                                                    | 0/84190 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/scratch/pawsey1001/haodongyang/loretta/bert_model/run_glue_v5.py", line 415, in <module>
    main()
  File "/scratch/pawsey1001/haodongyang/loretta/bert_model/run_glue_v5.py", line 355, in main
    trainer.train()
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/trainer.py", line 2241, in train
    return inner_training_loop(
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/trainer.py", line 2548, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/trainer.py", line 3740, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/accelerate/accelerator.py", line 2359, in backward
    loss.backward(**kwargs)
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [768, 768]] is at version 5; expected version 3 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
Traceback (most recent call last):
  File "/scratch/pawsey1001/haodongyang/loretta/bert_model/run_glue_v5.py", line 415, in <module>
    main()
  File "/scratch/pawsey1001/haodongyang/loretta/bert_model/run_glue_v5.py", line 355, in main
    trainer.train()
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/trainer.py", line 2241, in train
    return inner_training_loop(
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/trainer.py", line 2548, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/trainer.py", line 3740, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/accelerate/accelerator.py", line 2359, in backward
    loss.backward(**kwargs)
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [768, 768]] is at version 5; expected version 3 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).