Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
DebertaV2ForSequenceClassification(
  (deberta): DebertaV2Model(
    (embeddings): DebertaV2Embeddings(
      (word_embeddings): Embedding(128100, 768, padding_idx=0)
      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): DebertaV2Encoder(
      (layer): ModuleList(
        (0-5): 6 x DebertaV2Layer(
          (attention): DebertaV2Attention(
            (self): DisentangledSelfAttention(
              (query_proj): Linear(in_features=768, out_features=768, bias=True)
              (key_proj): Linear(in_features=768, out_features=768, bias=True)
              (value_proj): Linear(in_features=768, out_features=768, bias=True)
              (pos_dropout): Dropout(p=0.1, inplace=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): DebertaV2SelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): DebertaV2Intermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): DebertaV2Output(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (rel_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
    )
  )
  (pooler): ContextPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0, inplace=False)
  )
  (classifier): Linear(in_features=768, out_features=2, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
PeftModelForSequenceClassification(
  (base_model): LoraModel(
    (model): DebertaV2ForSequenceClassification(
      (deberta): DebertaV2Model(
        (embeddings): DebertaV2Embeddings(
          (word_embeddings): Embedding(128100, 768, padding_idx=0)
          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): DebertaV2Encoder(
          (layer): ModuleList(
            (0-5): 6 x DebertaV2Layer(
              (attention): DebertaV2Attention(
                (self): DisentangledSelfAttention(
                  (query_proj): lora.Linear(
                    (base_layer): Linear(in_features=768, out_features=768, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Identity()
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=768, out_features=8, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=8, out_features=768, bias=False)
                    )
                    (lora_E): ModuleDict(
                      (default): Linear(in_features=8, out_features=1, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                    (lora_magnitude_vector): ModuleDict()
                  )
                  (key_proj): Linear(in_features=768, out_features=768, bias=True)
                  (value_proj): lora.Linear(
                    (base_layer): Linear(in_features=768, out_features=768, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Identity()
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=768, out_features=8, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=8, out_features=768, bias=False)
                    )
                    (lora_E): ModuleDict(
                      (default): Linear(in_features=8, out_features=1, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                    (lora_magnitude_vector): ModuleDict()
                  )
                  (pos_dropout): Dropout(p=0.1, inplace=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): DebertaV2SelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): DebertaV2Intermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): DebertaV2Output(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (rel_embeddings): Embedding(512, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
        )
      )
      (pooler): ContextPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (dropout): Dropout(p=0, inplace=False)
      )
      (classifier): ModulesToSaveWrapper(
        (original_module): Linear(in_features=768, out_features=2, bias=True)
        (modules_to_save): ModuleDict(
          (default): Linear(in_features=768, out_features=2, bias=True)
        )
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)
name base_model.model.deberta.encoder.layer.0.attention.self.query_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.0.attention.self.query_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.0.attention.self.query_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.0.attention.self.value_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.0.attention.self.value_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.0.attention.self.value_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.1.attention.self.query_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.1.attention.self.query_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.1.attention.self.query_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.1.attention.self.value_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.1.attention.self.value_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.1.attention.self.value_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.2.attention.self.query_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.2.attention.self.query_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.2.attention.self.query_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.2.attention.self.value_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.2.attention.self.value_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.2.attention.self.value_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.3.attention.self.query_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.3.attention.self.query_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.3.attention.self.query_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.3.attention.self.value_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.3.attention.self.value_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.3.attention.self.value_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.4.attention.self.query_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.4.attention.self.query_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.4.attention.self.query_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.4.attention.self.value_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.4.attention.self.value_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.4.attention.self.value_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.5.attention.self.query_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.5.attention.self.query_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.5.attention.self.query_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.5.attention.self.value_proj.lora_A.default.weight shape torch.Size([768, 8]) dtype torch.float32
name base_model.model.deberta.encoder.layer.5.attention.self.value_proj.lora_B.default.weight shape torch.Size([8, 768]) dtype torch.float32
name base_model.model.deberta.encoder.layer.5.attention.self.value_proj.lora_E.default.weight shape torch.Size([8]) dtype torch.float32
name base_model.model.classifier.modules_to_save.default.weight shape torch.Size([2, 768]) dtype torch.float32
name base_model.model.classifier.modules_to_save.default.bias shape torch.Size([2]) dtype torch.float32
tokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 52.0/52.0 [00:00<00:00, 405kB/s]
spm.model: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.46M/2.46M [00:00<00:00, 53.2MB/s]
Traceback (most recent call last):
  File "/scratch/pawsey1001/haodongyang/loretta/bert_model/run_glue_v5.py", line 415, in <module>
    main()
  File "/scratch/pawsey1001/haodongyang/loretta/bert_model/run_glue_v5.py", line 240, in main
    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 963, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2052, in from_pretrained
    return cls._from_pretrained(
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2292, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py", line 103, in __init__
    super().__init__(
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 120, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1717, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 553, in __init__
    model_pb2 = import_protobuf()
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 38, in import_protobuf
    from sentencepiece import sentencepiece_model_pb2
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/sentencepiece/sentencepiece_model_pb2.py", line 34, in <module>
    _descriptor.EnumValueDescriptor(
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).
More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Traceback (most recent call last):
  File "/scratch/pawsey1001/haodongyang/loretta/bert_model/run_glue_v5.py", line 415, in <module>
    main()
  File "/scratch/pawsey1001/haodongyang/loretta/bert_model/run_glue_v5.py", line 240, in main
    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 963, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2052, in from_pretrained
    return cls._from_pretrained(
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2292, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py", line 103, in __init__
    super().__init__(
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 120, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1717, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 553, in __init__
    model_pb2 = import_protobuf()
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 38, in import_protobuf
    from sentencepiece import sentencepiece_model_pb2
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/sentencepiece/sentencepiece_model_pb2.py", line 34, in <module>
    _descriptor.EnumValueDescriptor(
  File "/scratch/pawsey1001/haodongyang/envs/envs/bert/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).
More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates